{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6 7 8]\n"
     ]
    }
   ],
   "source": [
    "# Part 1.1\n",
    "a = np.array([1,2,3,4,5,6,7,8])\n",
    "shape_a = a.reshape(2,4)\n",
    "\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6.,  9., 13., 15.])\n",
      "tensor([ 5., 18., 40., 54.])\n",
      "tensor([1.0000e+00, 7.2900e+02, 3.9062e+05, 1.0078e+07])\n",
      "tensor(117.)\n",
      "tensor(1235.4036)\n"
     ]
    }
   ],
   "source": [
    "# Part 1.2\n",
    "\n",
    "a = torch.tensor([1, 3, 5, 6], dtype=torch.float32)\n",
    "b = torch.tensor([5, 6, 8, 9], dtype=torch.float32)\n",
    "\n",
    "addition_result = a + b\n",
    "\n",
    "print(addition_result)\n",
    "\n",
    "multiplication_result = a * b\n",
    "\n",
    "print(multiplication_result)\n",
    "\n",
    "power_result = a ** b\n",
    "\n",
    "print(power_result)\n",
    "\n",
    "dot_product = torch.dot(a, b)\n",
    "\n",
    "print(dot_product)\n",
    "\n",
    "special_dot_prod = torch.dot(torch.exp(a), torch.log(b))\n",
    "\n",
    "print(special_dot_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.3 Written sections....\n",
    "\n",
    "\n",
    "$$\\frac{dg}{dx} = e^x (2x + x^2)$$\n",
    "$$\\frac{dg}{dy} = 3e^y (2y + y^2)$$\n",
    "$$\\frac{dg}{dz} = 5e^z (2z + z^2)$$\n",
    "$$\\frac{dg}{dk} = 6e^k (2k + k^2)$$\n",
    "\n",
    "$$\\frac{dg}{dx} = e^5 \\times 35$$\n",
    "$$\\frac{dg}{dy} = 3e^6 \\times 48$$\n",
    "$$\\frac{dg}{dz} = 5e^8 \\times 80$$\n",
    "$$\\frac{dg}{dk} = 6e^9 \\times 99$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dg/dx: 5194.4609375\n",
      "dg/dy: 58093.75\n",
      "dg/dz: 1192383.25\n",
      "dg/dk: 4813232.0\n"
     ]
    }
   ],
   "source": [
    "# part 1.3 (a)\n",
    "\n",
    "def g_function(x, y, z, k):\n",
    "    return torch.exp(x) * x**2 + 3 * torch.exp(y) * y**2 + 5 * torch.exp(z) * z**2 + 6 * torch.exp(k) * k**2\n",
    "\n",
    "# Initialize variables with autograd enabled\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "y = torch.tensor(6.0, requires_grad=True)\n",
    "z = torch.tensor(8.0, requires_grad=True)\n",
    "k = torch.tensor(9.0, requires_grad=True)\n",
    "\n",
    "# Compute the function value\n",
    "g_value = g_function(x, y, z, k)\n",
    "\n",
    "# Compute gradients\n",
    "g_value.backward()\n",
    "\n",
    "# Output gradients\n",
    "for i, name in enumerate(['x', 'y', 'z', 'k']):\n",
    "    variables = locals()[name]\n",
    "    print(f\"dg/d{name}: {variables.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.3 (b)\n",
    "$$f\\left(A\\right)\\ =\\ \\log\\left( \\lVert A^T AB^T AA^T AB^T \\rVert_2^2 \\right)$$\n",
    "$$f(A) = \\log \\left( \\lVert (A^T A)^2 B^T B \\rVert_2^2 \\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 part (b)\n",
    "\n",
    "# Enable autograd for tensor A\n",
    "A = torch.tensor([[4.0, 3.0], [7.0, 9.0]], requires_grad=True)\n",
    "B = torch.tensor([[3.0, 5.0], [1.0, 11.0]], requires_grad=False)\n",
    "\n",
    "# Compute the function f(A)\n",
    "AT = A.t()\n",
    "BT = B.t()\n",
    "term = torch.matmul(torch.matmul(AT, A), B.t())\n",
    "term = torch.matmul(torch.matmul(term, A), A.t())\n",
    "term = torch.matmul(term, B)\n",
    "L2_norm_squared = torch.norm(term, p=2)**2\n",
    "f_A = torch.log(L2_norm_squared)\n",
    "\n",
    "# Compute the gradient\n",
    "f_A.backward()\n",
    "\n",
    "# Output the gradient\n",
    "print(A.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 (C)\n",
    "\n",
    "$$F\\left(x,y\\right)\\ =\\ \\tanh\\left(x\\right)\\ +\\ \\tanh\\left(y\\right)$$\n",
    "\n",
    "Take the following derivatives: $$\\frac{dF}{dx}=1-\\tanh^2\\left(x\\right)$$\n",
    "$$\\frac{dF}{dx}=1-\\tanh^2\\left(y\\right)$$\n",
    "\n",
    "When x = 3.0, and y = 7.0 these derivatives should become quite small reaching close to 1, because of that $1-\\tanh^{2}(x)$ and $1-\\tanh^{2}(y)$ closes to zero. \\\n",
    "at the point ($x=3.0, y=7.0$)\n",
    "\n",
    "For the values when x = 3.0 we get: $$\\frac{dF}{dx}=1−\\tanh2(3)=1−0.99505475368673052\\approx0.0099$$\n",
    "For the values when y = 7.0  we get: $$\\frac{dF}{dy}=1−\\tanh2(7)=1−0.99999833694394512\\approx0.00000334$$\n",
    "\n",
    "(Similar to what our code below is generating using pytorch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient with respect to x is 0.009865999221801758\n",
      "The gradient with respect to y is 3.337860107421875e-06\n"
     ]
    }
   ],
   "source": [
    "# Initialize x and y as tensors and set them to require gradients\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = torch.tensor(7.0, requires_grad=True)\n",
    "\n",
    "# Define the function F(x, y)\n",
    "F = torch.tanh(x) + torch.tanh(y)\n",
    "\n",
    "# Compute the gradients\n",
    "F.backward()\n",
    "\n",
    "# Get the gradients\n",
    "grad_x = x.grad.item()\n",
    "grad_y = y.grad.item()\n",
    "\n",
    "print(f\"The gradient with respect to x is {grad_x}\")\n",
    "print(f\"The gradient with respect to y is {grad_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.4 \n",
    "\n",
    "* Let a be a torch integer tensor containing the values [1, 2, 3].\n",
    "* Convert a to a numpy array and store it under a new variable b\n",
    "* convert a into a float tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor a: tensor([1, 2, 3], dtype=torch.int32)\n",
      "Numpy array b: [1 2 3]\n",
      "Float tensor a_float: tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an integer tensor 'a'\n",
    "a = torch.tensor([1, 2, 3], dtype=torch.int)\n",
    "\n",
    "# Convert 'a' to a numpy array 'b'\n",
    "b = np.asarray(a)\n",
    "\n",
    "# Convert 'a' into a float tensor\n",
    "a_float = a.to(dtype=torch.float)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Original tensor a: {a}\")\n",
    "print(f\"Numpy array b: {b}\")\n",
    "print(f\"Float tensor a_float: {a_float}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Answer the following questions using the package Numpy:\n",
    "    * What is the product of matrices of matrices [[1, 3, 5], [2, 1, 5]] and [[8, 4], [3, 6], [2, 7]]?\n",
    "\n",
    "\n",
    "    * What is the Frobenius norm of the 1 x 3 matrix [100, 2, 1]?\n",
    "    Forbenius Norm is given by $\\sqrt{trace(A^TA)}$\n",
    "    $$\\sqrt{100^2+2^2+1}$$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27 57]\n",
      " [29 49]] This is the produce of the matrices given\n",
      "100.024996875781 This is the Frobenius Norm of the 1x3 matrix.\n",
      "100.024996875781 This is the Frobenius Norm as calculated (See above for exact calculation)\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 3, 5], [2, 1, 5]])\n",
    "B = np.array([[8, 4], [3, 6], [2, 7]])\n",
    "\n",
    "result = np.matmul(A, B)\n",
    "print(f\"{result} This is the product of the matrices given\")\n",
    "\n",
    "C = np.array([[100, 2, 1]])  # Make into 1x3 matrix\n",
    "frobenius_norm = np.linalg.norm(C, ord='fro')\n",
    "print(f\"{frobenius_norm} This is the Frobenius Norm of the 1x3 matrix.\")\n",
    "\n",
    "print(f\"{frobenius_norm} This is the Frobenius Norm as calculated (See above for exact calculation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
