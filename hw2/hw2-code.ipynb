{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX0ia6JVtFjr",
        "outputId": "7e136dd8-f1a6-49cc-967c-357300efec61"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/msturman00/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/msturman00/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# import packages\n",
        "import os\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import emoji\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZtYd4rVNQqy"
      },
      "source": [
        "# Problem 1\n",
        "## Data generating distribution and convergence of linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD0dQabauB7z"
      },
      "source": [
        "# Part (a): Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "uRBDrBxYtBbk"
      },
      "outputs": [],
      "source": [
        "#====================================================#\n",
        "# YOUR CODE HERE:\n",
        "#   Import train and test csv files.\n",
        "#   You should use the pd.read_csv function.\n",
        "#   You should set the index_col parameter to equal 'id'.\n",
        "#====================================================#\n",
        "\n",
        "train_data_path = './data/train.csv'\n",
        "test_data_path = './data/test.csv'\n",
        "# Read data and set index column\n",
        "train_data = pd.read_csv(train_data_path, index_col='id')\n",
        "test_data  = pd.read_csv(test_data_path, index_col='id')\n",
        "\n",
        "#====================================================#\n",
        "# END YOUR CODE\n",
        "#====================================================#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgB6rvFLNQq2",
        "outputId": "26e8d750-8f05-4dc7-cb0a-089bc60c7081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['keyword', 'location', 'text', 'target'], dtype='object')\n",
            "Index(['keyword', 'location', 'text'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(train_data.columns)\n",
        "print(test_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG7kzzuDvlBr",
        "outputId": "63d1d4ea-c72c-4b0a-9859-2a2847be31f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data Shape: (7613, 3)\n",
            "Test Data Shape: (3263, 3)\n",
            "Number of labels = 1 in train dataset as percentage: 42.97%\n",
            "Number of labels = 0 in train dataset as percentage: 57.03%\n"
          ]
        }
      ],
      "source": [
        "#====================================================#\n",
        "# YOUR CODE HERE:\n",
        "#   Get the index values for X_train and y_train.\n",
        "#   Get the data values for X_train and y_train.\n",
        "#   Get the index values for X_test.\n",
        "#   Get the index values for y_test.\n",
        "#====================================================#\n",
        "\n",
        "# Get train indices\n",
        "X_train_id = train_data.index\n",
        "Y_train_id = train_data.index\n",
        "\n",
        "# Get train data\n",
        "X_train = train_data[['keyword', 'location', 'text']]\n",
        "Y_train = train_data['target']\n",
        "\n",
        "# Get test indices\n",
        "X_test_id = test_data.index\n",
        "\n",
        "# Get test data\n",
        "X_test = test_data[['keyword', 'location', 'text']]\n",
        "\n",
        "\n",
        "#====================================================#\n",
        "# END YOUR CODE\n",
        "#====================================================#\n",
        "\n",
        "print(f\"Train Data Shape: {X_train.shape}\")\n",
        "print(f\"Test Data Shape: {X_test.shape}\")\n",
        "\n",
        "print(f\"Number of labels = 1 in train dataset as percentage: {((Y_train == 1).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
        "print(f\"Number of labels = 0 in train dataset as percentage: {((Y_train == 0).sum() / (X_train.shape[0])) * 100:0.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_16NkNGt3Kx"
      },
      "source": [
        "### Part (a), Question 1: How many training and test data points are there?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uqBPKEYt7W5"
      },
      "source": [
        "### Answer: There are total of 7613 training data points and 3263 test data points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEVclzmqt9TF"
      },
      "source": [
        "### Part (a), Question 2: what percentage of the training tweets are of real disasters, and what percentage is not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J7911HiucTb"
      },
      "source": [
        "### Answer: 42.97% is real disasters and 57.03% is not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrzjUIXfuHEt"
      },
      "source": [
        "# Part (b): Split the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy51Q0txuJpC",
        "outputId": "98bc5944-974e-473d-c5cb-fb90c796bdd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train_orig shape:  (5329, 3)\n",
            "X_develop_orig shape:  (2284, 3)\n",
            "y_train_orig shape:  (5329,)\n",
            "y_develop_orig shape:  (2284,)\n"
          ]
        }
      ],
      "source": [
        "#====================================================#\n",
        "# YOUR CODE HERE:\n",
        "#  You should use the sklearn.model_selection.train_test_split\n",
        "#     parameter to perform the train/development split\n",
        "#   Set the test_size to 0.30.\n",
        "#   Set the random_stat parameter to 42.\n",
        "#====================================================#\n",
        "\n",
        "X_train_orig, X_develop_orig, y_train_orig, y_develop_orig = train_test_split(train_data[['keyword', 'location', 'text']], train_data['target'], test_size=0.30, random_state=42)\n",
        "# Output the result\n",
        "print(\"X_train_orig shape: \", X_train_orig.shape)\n",
        "print(\"X_develop_orig shape: \", X_develop_orig.shape)\n",
        "print(\"y_train_orig shape: \", y_train_orig.shape)\n",
        "print(\"y_develop_orig shape: \", y_develop_orig.shape)\n",
        "\n",
        "# Save the training data into a csv file\n",
        "train_data_to_save = X_train_orig.copy()\n",
        "train_data_to_save['target'] = y_train_orig\n",
        "train_data_to_save.to_csv('train_data.csv', index=False)\n",
        "\n",
        "# Save the development data into a csv file\n",
        "develop_data_to_save = X_develop_orig.copy()\n",
        "develop_data_to_save['target'] = y_develop_orig\n",
        "develop_data_to_save.to_csv('develop_data.csv', index=False)\n",
        "#====================================================#\n",
        "# END YOUR CODE\n",
        "#====================================================#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSQOZqupuKGO"
      },
      "source": [
        "# Part (c): Preprocess the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWpMOdJ2uipq",
        "outputId": "b0c69e93-8d84-4430-9b0b-9ad0296382b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessed X_train data:\n",
            "                 keyword                location  \\\n",
            "id                                                 \n",
            "1707   bridge%20collapse                     NaN   \n",
            "5789                hail  Carol Stream, Illinois   \n",
            "7789              police                Houston    \n",
            "8257             rioting                     NaN   \n",
            "10656             wounds          Lake Highlands   \n",
            "\n",
            "                                                    text  \n",
            "id                                                        \n",
            "1707   ash 2015 australiaûªs collapse trent bridge am...  \n",
            "5789   great michigan technique camp b1g thanks bmurp...  \n",
            "7789   cnn tennessee movie theater shooting suspect k...  \n",
            "8257                still rioting couple hour left class  \n",
            "10656  crack path wiped morning beach run surface wou...  \n",
            "Preprocessed X_develop data:\n",
            "          keyword               location  \\\n",
            "id                                         \n",
            "3796  destruction                    NaN   \n",
            "3185       deluge                    NaN   \n",
            "7769       police                     UK   \n",
            "191    aftershock                    NaN   \n",
            "9810       trauma  Montgomery County, MD   \n",
            "\n",
            "                                                   text  \n",
            "id                                                       \n",
            "3796          new weapon cause unimaginable destruction  \n",
            "3185  famping thing gishwhes got soaked deluge going...  \n",
            "7769  dt georgegalloway rt galloway4mayor ûïthe col ...  \n",
            "191   aftershock back school kick wa great want than...  \n",
            "9810  response trauma child addict develop defensive...  \n"
          ]
        }
      ],
      "source": [
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  You should complete the following function to obtain the pre-processed\n",
        "#  X_train and X_develop\n",
        "#  Note that we suggest you to do every sub-question in a dedicated Python\n",
        "#  function to make the code more structured and less error-prone.\n",
        "#  With a function, you can clearly test each part when you encounter an error.\n",
        "#  You can also create your own simple input data (e.g. just one sample) to\n",
        "#  test the correctness of a function.\n",
        "\n",
        "#========================================================================#\n",
        "def pre_process(data):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def clean_text(text):\n",
        "        # Convert all the words to lowercase (Helps in achieving uniformity)\n",
        "        text = text.lower()\n",
        "\n",
        "        # Lemmatize all the words (Aids in reducing the dimensionality of the data and capturing semantic meanings)\n",
        "        text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "        # Strip punctuation (Removes unnecessary noise from the data)\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "        # Strip HTML tags\n",
        "        text = re.sub(r'<.*?>','',text)\n",
        "\n",
        "        # Strip the stop words (Helps in focusing on significant words, though sometimes it might remove context)\n",
        "        text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "        # Strip @ and URLs (Helps in removing references to users and external links which might not be necessary for analysis)\n",
        "        text = re.sub(r'@\\w+|http\\S+', '', text)\n",
        "\n",
        "        # remove emoji\n",
        "        text = emoji.replace_emoji(text,'')\n",
        "\n",
        "        return text\n",
        "\n",
        "    data['text'] = data['text'].apply(clean_text)\n",
        "    return data # Feel free to change the variable name\n",
        "    #========================================================================#\n",
        "    #  This function should return the pre-processed data\n",
        "    #========================================================================#\n",
        "\n",
        "# get the preprocessed data\n",
        "# Get the preprocessed data\n",
        "X_train_preproc   = pre_process(X_train_orig)\n",
        "X_develop_preproc = pre_process(X_develop_orig)\n",
        "\n",
        "# Output the result (if you want to check the initial results)\n",
        "print(\"Preprocessed X_train data:\")\n",
        "print(X_train_preproc.head())\n",
        "\n",
        "print(\"Preprocessed X_develop data:\")\n",
        "print(X_develop_preproc.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GhiuEjdumEO"
      },
      "source": [
        "# Part (d): Bag of words model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "bNaNqYjkNQq6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bound method spmatrix.get_shape of <2284x13708 sparse matrix of type '<class 'numpy.int64'>'\n",
            "\twith 16979 stored elements in Compressed Sparse Row format>>\n",
            "<bound method spmatrix.get_shape of <5329x13708 sparse matrix of type '<class 'numpy.int64'>'\n",
            "\twith 48708 stored elements in Compressed Sparse Row format>>\n"
          ]
        }
      ],
      "source": [
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  You should complete the following function to obtain X_train and X_develop,\n",
        "#  whose \"text\" feature only contains 1 and 0 to indicate whether a word is in\n",
        "#  the tweet. At this point, you should only be constructing feature vectors\n",
        "#  for each data point using the text in the “text” column.\n",
        "#  You should ignore the “keyword” and “location” columns for now.\n",
        "#========================================================================#\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "X_train_cv = vectorizer.fit_transform(X_train_preproc['text'])\n",
        "X_develop_cv = vectorizer.transform(X_develop_preproc['text'])\n",
        "feature_names = vectorizer.vocabulary_.keys()\n",
        "\n",
        "print(X_develop_cv.get_shape)\n",
        "print(X_train_cv.get_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReEOC-rkupXV"
      },
      "source": [
        "# Part (e): Logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpjvSzoduu3O",
        "outputId": "ba2fc496-21f2-4d1a-b385-6bb281776485"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 for training set: 0.99\n",
            "F1 for development set: 0.71\n"
          ]
        }
      ],
      "source": [
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  You should complete the following function for logistic regression\n",
        "#  with L2 regularization.\n",
        "#  You will be training logistic regression models using bag of words\n",
        "#  feature vectors obtained in part (d).\n",
        "#========================================================================#\n",
        "\n",
        "\n",
        "def logistic_without_regularization(X_train, Y_train, X_develop):\n",
        "    # Initialize your logistic regression model\n",
        "    model = LogisticRegression(penalty='none', max_iter=1000)\n",
        "\n",
        "    # Fit your model to the train data\n",
        "    model.fit(X_train, Y_train)\n",
        "\n",
        "    # Generate your prediction for the training set\n",
        "    y_train_no_reg = model.predict(X_train)\n",
        "\n",
        "    # Generate your prediction for the development set\n",
        "    y_develop_no_reg = model.predict(X_develop)\n",
        "    #========================================================================#\n",
        "    #  This function should train a logistic regression model without\n",
        "    #  regularization terms.\n",
        "    #  Report the F1 score in your training and in your development sets.\n",
        "    #========================================================================#\n",
        "    # Return the predictions\n",
        "    return y_train_no_reg, y_develop_no_reg\n",
        "\n",
        "# Call the function and get predictions\n",
        "y_train_no_reg, y_develop_no_reg = logistic_without_regularization(X_train_cv, y_train_orig, X_develop_cv)\n",
        "\n",
        "# Get the F1 train and develop scores\n",
        "F1_train_no_reg = f1_score(y_train_orig, y_train_no_reg)\n",
        "F1_develop_no_reg = f1_score(y_develop_orig, y_develop_no_reg)\n",
        "\n",
        "# Print the F1 train and develop scores\n",
        "print(f\"F1 for training set: {F1_train_no_reg:.2f}\")\n",
        "print(f\"F1 for development set: {F1_develop_no_reg:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaZwAJP6KUfv",
        "outputId": "a3175dd7-5ede-4589-d849-ac48943c74bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 for training set: 0.85\n",
            "F1 for development set: 0.74\n"
          ]
        }
      ],
      "source": [
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  You should complete the following function for logistic regression\n",
        "#  with L1 regularization.\n",
        "#  You will be training logistic regression models using bag of words\n",
        "#  feature vectors obtained in part (d).\n",
        "#========================================================================#\n",
        "def logistic_L1_regularization(X_train, Y_train, X_develop):\n",
        "    # initialize your logistic regression model\n",
        "    clf = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
        "\n",
        "    # then fit your model to the train data\n",
        "    clf.fit(X_train, Y_train)\n",
        "\n",
        "    # then generate your prediction for the training set\n",
        "    y_train_L1_reg = clf.predict(X_train)\n",
        "\n",
        "    # then generate your prediction for the development set\n",
        "    y_develop_L1_reg = clf.predict(X_develop)\n",
        "    #========================================================================#\n",
        "    #  This function should train a logistic regression model without\n",
        "    #  regularization terms.\n",
        "    #  Report the F1 score in your training and in your development sets.\n",
        "    #========================================================================#\n",
        "    return y_train_L1_reg, y_develop_L1_reg,clf.coef_[0]\n",
        "\n",
        "# Call the function and get predictions\n",
        "y_train_l1_reg, y_develop_l1_reg, L1_coef = logistic_L1_regularization(X_train_cv, y_train_orig, X_develop_cv)\n",
        "\n",
        "# get the F1 train and develop scores\n",
        "F1_train_L1_reg = f1_score(y_train_orig, y_train_l1_reg)\n",
        "F1_develop_L1_reg = f1_score(y_develop_orig, y_develop_l1_reg)\n",
        "\n",
        "# print the F1 train and develop scores\n",
        "print(f\"F1 for training set: {F1_train_L1_reg:.2f}\")\n",
        "print(f\"F1 for development set: {F1_develop_L1_reg:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PHKy8ElKVAg",
        "outputId": "e9d335cb-0c0b-4f7f-c2c9-f8ebd35c929c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 for training set: 0.96\n",
            "F1 for development set: 0.75\n"
          ]
        }
      ],
      "source": [
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  You should complete the following function for logistic regression\n",
        "#  with L2 regularization.\n",
        "#  You will be training logistic regression models using bag of words\n",
        "#  feature vectors obtained in part (d).\n",
        "#========================================================================#\n",
        "def logistic_L2_regularization(X_train, Y_train, X_develop):\n",
        "    # initialize your logistic regression model\n",
        "    clf = LogisticRegression(penalty='l2', max_iter=1000)\n",
        "\n",
        "    # then fit your model to the train data\n",
        "    clf.fit(X_train, Y_train)\n",
        "    # then fit your model to the train data\n",
        "\n",
        "    # then generate your prediction for the training set\n",
        "    y_train_L2_reg = clf.predict(X_train)\n",
        "\n",
        "    # then generate your prediction for the development set\n",
        "    y_develop_L2_reg = clf.predict(X_develop)\n",
        "    #========================================================================#\n",
        "    #  This function should train a logistic regression model without\n",
        "    #  regularization terms.\n",
        "    #  Report the F1 score in your training and in your development sets.\n",
        "    #========================================================================#\n",
        "    return y_train_L2_reg, y_develop_L2_reg\n",
        "\n",
        "\n",
        "# Call the function and get predictions\n",
        "y_train_l2_reg, y_develop_l2_reg = logistic_L2_regularization(X_train_cv, y_train_orig, X_develop_cv)\n",
        "\n",
        "# get the F1 train and develop scores\n",
        "F1_train_L2_reg = sklearn.metrics.f1_score(y_train_orig, y_train_l2_reg)\n",
        "F1_develop_L2_reg = sklearn.metrics.f1_score(y_develop_orig, y_develop_l2_reg)\n",
        "\n",
        "# print the F1 train and develop scores\n",
        "print(f\"F1 for training set: {F1_train_L2_reg:.2f}\")\n",
        "print(f\"F1 for development set: {F1_develop_L2_reg:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DmYQSKkK_L3"
      },
      "source": [
        "### Which one of the three classifiers performed the best on your training and development set? Did you observe any overfitting and did regularization help reduce it? Support your answers with the classifier performance you got."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYpWFNElLbWv"
      },
      "source": [
        ">  Answer:\n",
        "- Logistic Regression with L2 Regularization was the best model with the following scores:\n",
        "  -  Training Set F1-score: 0.96\n",
        "  -  Development Set F1-score: 0.75\n",
        "- We can see overfitting in the model without Regularization as it achevies the following scores:\n",
        "  -  Training Set F1-score: 0.99\n",
        "  -  Development Set F1-score: 0.71\n",
        "  the large gap between the training and development scores showing ovefitting.\n",
        "- L1 and L2 regularization reduced the gap between the training and development scores which means they helped in reducing the overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yysent_nLuh1"
      },
      "source": [
        "### Inspect the weight vector of the classifier with L1 regularization (in other words, look at the θ you got after training). You can access the weight vector of the trained model using the coef_attribute of a LogisticRegression instance. What are the most important words for deciding whether a tweet is about a real disaster or not? You might need to run some code (feel free to insert a code cell below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euz3Gc-Sele_",
        "outputId": "d5fb3c64-6e8a-4de9-c049-0fda0c97304b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lights: 3.5567371130928795\n",
            "details: 3.390702140726911\n",
            "deckû: 3.3906462091162823\n",
            "mark: 3.3180917621128536\n",
            "quarantined: 3.170928425695592\n",
            "allergic: 3.0405325998245254\n",
            "pizza: 2.9602764812876\n",
            "alive: 2.869710359648432\n",
            "sequence: 2.810220144570299\n",
            "pickerel: 2.7939519250342593\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create a dictionary to associate feature names with their coefficients\n",
        "word_coefficient_dict = {}\n",
        "for word, coef in zip(feature_names, L1_coef):\n",
        "    word_coefficient_dict[word] = coef\n",
        "\n",
        "# Sort the words by their coefficients in descending order\n",
        "important_words = sorted(word_coefficient_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the top N important words\n",
        "top_n = 10  # Change this to the number of top words you want to display\n",
        "for word, coef in important_words[:top_n]:\n",
        "    print(f\"{word}: {coef}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ2S1QVCLwuY"
      },
      "source": [
        "> Answer: the top 10 most important words are: lights, details, deckû, mark, quarantined, allergic, pizza, alive, sequence and pickerel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UWCsGNruvXD"
      },
      "source": [
        "# Part (f): Bernoulli Naive Bayes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg7nMMI3u0HY",
        "outputId": "29b7c0a2-23c4-4441-f5a1-ee7673efd9ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 for training set: 0.91\n",
            "F1 for development set: 0.74\n"
          ]
        }
      ],
      "source": [
        "class BernoulliNB:\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "        self.class_probs = None\n",
        "        self.feature_probs = None\n",
        "        #====================================================#\n",
        "        # YOUR CODE HERE:\n",
        "        #  You should build the Bernoully NB model from scratch\n",
        "        #  Do not use sklearn, use numpy and other basic packages\n",
        "        #    only.\n",
        "        #  Please update and save the parameters\n",
        "        #    \"self.class_log_prior_\" and \"self.feature_prob_\"\n",
        "        #  These variables are just a suggestion to help\n",
        "        #    structure your code - you do not need to use them\n",
        "        #    if you would prefer not to\n",
        "        #====================================================#\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "\n",
        "        # Calculate class probabilities\n",
        "        self.class_probs = np.zeros(n_classes)\n",
        "        for i, class_label in enumerate(self.classes):\n",
        "            self.class_probs[i] = (np.sum(y == class_label) + self.alpha) / (n_samples + self.alpha * n_classes)\n",
        "\n",
        "        # Calculate feature probabilities for each class\n",
        "        self.feature_probs = np.zeros((n_classes, n_features))\n",
        "        for i, class_label in enumerate(self.classes):\n",
        "            class_samples = X[y == class_label]\n",
        "            self.feature_probs[i] = (np.sum(class_samples, axis=0) + self.alpha) / (len(class_samples) + self.alpha * 2)\n",
        "    #====================================================#\n",
        "    # END YOUR CODE\n",
        "    #====================================================#\n",
        "    \n",
        "    #====================================================#\n",
        "    # YOUR CODE HERE:\n",
        "    #  You should build the Bernoully NB model from scratch\n",
        "    #  Do not use sklearn, use numpy and other basic packages\n",
        "    #    only.\n",
        "    #  Please update and save the parameters\n",
        "    #    \"self.pred_log_prob_\" and \"y_pred\"\n",
        "    #  These variables are just a suggestion to help\n",
        "    #    structure your code - you do not need to use them\n",
        "    #    if you would prefer not to\n",
        "    #====================================================#\n",
        "    def predict(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_classes = len(self.classes)\n",
        "        predictions = np.zeros((n_samples, n_classes))\n",
        "\n",
        "        for i in range(n_classes):\n",
        "            class_prob = np.log(self.class_probs[i])\n",
        "            feature_prob = np.log(self.feature_probs[i])\n",
        "            class_log_prior = np.log(1 - self.class_probs[i])\n",
        "\n",
        "            predictions[:, i] = class_prob + np.sum(X * feature_prob, axis=1) + class_log_prior\n",
        "\n",
        "        return self.classes[np.argmax(predictions, axis=1)]\n",
        "    #====================================================#\n",
        "    # END YOUR CODE\n",
        "    #====================================================#\n",
        "def BernoulliNB_predict(X_train, Y_train, X_develop):\n",
        "      # get the predictions y_train_NB and y_develop_NB\n",
        "      clf = BernoulliNB()\n",
        "      clf.fit(X_train.toarray(), Y_train)\n",
        "\n",
        "      y_train_NB = clf.predict(X_train.toarray()) # prediction from X_train using model\n",
        "      y_develop_NB = clf.predict(X_develop.toarray()) # prediction from X_develop using model\n",
        "\n",
        "      return y_train_NB, y_develop_NB\n",
        "\n",
        "y_train_NB, y_develop_NB = BernoulliNB_predict(X_train_cv,y_train_orig,X_develop_cv)\n",
        "\n",
        "# get the F1 train and develop scores\n",
        "F1_train_NB = sklearn.metrics.f1_score(y_train_orig, y_train_NB)\n",
        "F1_develop_NB = sklearn.metrics.f1_score(y_develop_orig, y_develop_NB)\n",
        "\n",
        "# print the F1 train and develop scores\n",
        "print(f\"F1 for training set: {F1_train_NB:.2f}\")\n",
        "print(f\"F1 for development set: {F1_develop_NB:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JQrNzE6u0jY"
      },
      "source": [
        "# Part (g): Model comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7BcX1e7Ti0J"
      },
      "source": [
        "## Question: Which model performed the best in predicting whether a tweet is of a real disaster or not? Include your performance metric in your response. Comment on the pros and cons of using generative vs discriminative models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7NDJXWFTn4m"
      },
      "source": [
        "> Answer:\n",
        "- logistic regression model with L2 regularization was the best model based on F1-score, the logistic regression with L2 regularization acheived the highest F1-score on the development set = 0.75.\n",
        "-  generative vs. discriminative models:\n",
        " - **Generative Models (Bernoulli Naive Bayes):**\n",
        "    * pros: simple and provide clear insights into how features contribute to the classification decision. Have fast training time which make them suitable for large datasets.\n",
        "    * cons: Assumes that all features are conditionally independent, which may not true in all real-world scenarios, including NLP tasks.\n",
        " - **Discriminative Models (Logistic Regression):**  \n",
        "    * pros: explicitly model the relationship between features and the target variable, allowing them to capture complex dependencies in the data which make them powerful and acheive high accuracy with complex data.\n",
        "    * cons: more complex and require large amount of datafor training. Less interpretable, it's hard to understand how the prediction is made.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTrNTtLgTpFM"
      },
      "source": [
        "Question: think about the assumptions that Naive Bayes makes. How are the assumptions different from logistic regressions? Discuss whether it is valid and efficient to use Bernoulli Naive Bayes classifier for natural language texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvS8MuqBTqIR"
      },
      "source": [
        "> Answer:\n",
        "-  Naive Bayes assumes that all features are conditionally independent, which means in our case that the presence or absence of one word does not provide any information about the presence or absence of other words in the document.\n",
        "- Logistic regression, on the other hand, models the relationship between the words and the class label directly using a logistic function.\n",
        "- As the assumption of feature independence is not true in NLP tasks where words have complex relationship and dependencies. we cannot use Bernoulli\n",
        "Naive Bayes. but, with some representations like bag-of-words, BernoulliNaive Bayes can capture basic relationships between the presence or absence of words and the probability of a particular class.\n",
        "-  Bernoulli Naive Bayes is computationally efficient with large datasets which make it a good choice for many NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46y89mf2u4q2"
      },
      "source": [
        "# Part (h): N-gram model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hMAIW_jvkKM9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the N-gram range (e.g., for bigrams, set ngram_range=(2, 2))\n",
        "ngram_range = (2, 2)\n",
        "\n",
        "# Create a CountVectorizer with custom N-gram range\n",
        "vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
        "\n",
        "# Fit and transform the corpus to get the N-gram features\n",
        "X_train_2gram = vectorizer.fit_transform(X_train_preproc['text'])\n",
        "X_develop_2gram = vectorizer.transform(X_develop_preproc['text'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7q6X9Geu8NB",
        "outputId": "dcb803e6-35b2-47a2-c5bf-5fa0e7c5fe13"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 for training set: 0.98\n",
            "F1 for development set: 0.67\n",
            "F1 for training set: 0.63\n",
            "F1 for development set: 0.51\n",
            "F1 for training set: 0.98\n",
            "F1 for development set: 0.56\n",
            "F1 for training set: 0.98\n",
            "F1 for development set: 0.67\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  Use the functions you already defined \"X_train_gram\" and \"X_develop_gram\"\n",
        "#  to re-run:\n",
        "#  Logistic Regression with no regularization Model\n",
        "#  Logistic Regression with L1 regularization Model\n",
        "#  Logistic Regression with L2 regularization Model\n",
        "#========================================================================#\n",
        "y_train_gram_no_reg, y_develop_gram_no_reg = logistic_without_regularization(X_train_2gram, y_train_orig, X_develop_2gram)\n",
        "y_train_gram_L1_reg, y_develop_gram_L1_reg,F1_coef = logistic_L1_regularization(X_train_2gram, y_train_orig, X_develop_2gram)\n",
        "y_train_gram_L2_reg, y_develop_gram_L2_reg = logistic_L2_regularization(X_train_2gram, y_train_orig, X_develop_2gram)\n",
        "y_train_gram_NB, y_develop_gram_NB = BernoulliNB_predict(X_train_2gram,y_train_orig,X_develop_2gram)\n",
        "#========================================================================#\n",
        "# END CODE HERE\n",
        "#========================================================================#\n",
        "\n",
        "# get the F1 train and develop scores for no regularization model\n",
        "F1_train_gram_no_reg = sklearn.metrics.f1_score(y_train_orig, y_train_gram_no_reg)\n",
        "F1_develop_gram_no_reg = sklearn.metrics.f1_score(y_develop_orig, y_develop_gram_no_reg)\n",
        "# get the F1 train and develop scores for L1 regularization model\n",
        "F1_train_gram_L1_reg = sklearn.metrics.f1_score(y_train_orig, y_train_gram_L1_reg)\n",
        "F1_develop_gram_L1_reg = sklearn.metrics.f1_score(y_develop_orig, y_develop_gram_L1_reg)\n",
        "# get the F1 train and develop scores for L2 regularization model\n",
        "F1_train_gram_L2_reg = sklearn.metrics.f1_score(y_train_orig, y_train_gram_L2_reg)\n",
        "F1_develop_gram_L2_reg = sklearn.metrics.f1_score(y_develop_orig, y_develop_gram_L2_reg)\n",
        "# get the F1 train and develop scores for Bernoulli NB model\n",
        "F1_train_gram_NB = sklearn.metrics.f1_score(y_train_orig, y_train_gram_NB)\n",
        "F1_develop_gram_NB = sklearn.metrics.f1_score(y_develop_orig, y_develop_gram_NB)\n",
        "\n",
        "# print the F1 train and develop scores for no regularization model\n",
        "print(f\"F1 for training set: {F1_train_gram_NB:.2f}\")\n",
        "print(f\"F1 for development set: {F1_develop_gram_NB:.2f}\")\n",
        "# print the F1 train and develop scores for L1 regularization model\n",
        "print(f\"F1 for training set: {F1_train_gram_L1_reg:.2f}\")\n",
        "print(f\"F1 for development set: {F1_develop_gram_L1_reg:.2f}\")\n",
        "# print the F1 train and develop scores for L2 regularization model\n",
        "print(f\"F1 for training set: {F1_train_gram_L2_reg:.2f}\")\n",
        "print(f\"F1 for development set: {F1_develop_gram_L2_reg:.2f}\")\n",
        "# print the F1 train and develop scores for Bernoulli NB model\n",
        "print(f\"F1 for training set: {F1_train_gram_NB:.2f}\")\n",
        "print(f\"F1 for development set: {F1_develop_gram_NB:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55QmSb3ku8oa"
      },
      "source": [
        "# Part (i): Determine performance with the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i270_AqVvAbH",
        "outputId": "e1acefd6-747d-4ace-ba82-f96cb455ddc0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nr/9dz9m8pd2752bz2rmpkbrcs40000gn/T/ipykernel_50059/1622737045.py:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['text'] = data['text'].apply(clean_text)\n"
          ]
        }
      ],
      "source": [
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  Re-build your feature vectors on the entire Kaggle train set\n",
        "#  (i.e. DO NOT split the train set into a further train set and development set)\n",
        "#========================================================================#\n",
        "X_train_preproc   = pre_process(X_train)\n",
        "X_test_preproc    = pre_process(X_test)\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "X_train_cv_full = vectorizer.fit_transform(X_train_preproc['text'])\n",
        "X_test_cv = vectorizer.transform(X_test_preproc['text'])\n",
        "\n",
        "#========================================================================#\n",
        "# END CODE HERE\n",
        "#========================================================================#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u985um3AY1ie",
        "outputId": "5088e6ce-3942-455a-af2d-4af4f720edd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 for training set: 0.95\n"
          ]
        }
      ],
      "source": [
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  Re-train your preferred classifier (see below) on the entire train set\n",
        "#  (i.e. DO NOT split the train set into a further train set and development set)\n",
        "#  Your preferred classifier may inculde either bag of word or n-gram,\n",
        "#  and using either logistic regression or Bernoulli naive bayes\n",
        "#========================================================================#\n",
        "\n",
        "y_train_l2_reg_full, y_test_l2_reg = logistic_L2_regularization(X_train_cv_full, Y_train, X_test_cv)\n",
        "\n",
        "# get the F1 train and develop scores\n",
        "F1_train_L2_reg = sklearn.metrics.f1_score(Y_train, y_train_l2_reg_full)\n",
        "\n",
        "# print the F1 train and develop scores\n",
        "print(f\"F1 for training set: {F1_train_L2_reg:.2f}\")\n",
        "#========================================================================#\n",
        "# END CODE HERE\n",
        "#========================================================================#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6CIKyutPY1zL"
      },
      "outputs": [],
      "source": [
        "#=======================================================================+#\n",
        "# YOUR CODE HERE:\n",
        "#  Report the resulting F 1-score on the test data, as reported by Kaggle\n",
        "#========================================================================#\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'id': X_test_id,\n",
        "    'target': y_test_l2_reg\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "# predictions to be reported to kaggle\n",
        "#========================================================================#\n",
        "# END CODE HERE\n",
        "#========================================================================#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Submission photo Kaggle\n",
        "\n",
        "<img src=\"./Kaggle_Disaster.jpg\" width=\"%50\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
