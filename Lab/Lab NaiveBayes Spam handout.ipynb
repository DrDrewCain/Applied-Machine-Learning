{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0506af7f",
   "metadata": {},
   "source": [
    "Load the emails dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af2c5f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d27cb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  spam\n",
      "0     Subject: naturally irresistible your corporate...     1\n",
      "1     Subject: the stock trading gunslinger  fanny i...     1\n",
      "2     Subject: unbelievable new homes made easy  im ...     1\n",
      "3     Subject: 4 color printing special  request add...     1\n",
      "4     Subject: do not have money , get software cds ...     1\n",
      "...                                                 ...   ...\n",
      "5723  Subject: re : research and development charges...     0\n",
      "5724  Subject: re : receipts from visit  jim ,  than...     0\n",
      "5725  Subject: re : enron case study update  wow ! a...     0\n",
      "5726  Subject: re : interest  david ,  please , call...     0\n",
      "5727  Subject: news : aurora 5 . 2 update  aurora ve...     0\n",
      "\n",
      "[5728 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "email_path = './emails.csv'\n",
    "# Read data and set index column\n",
    "email = pd.read_csv(email_path)\n",
    "\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36000529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "5728\n"
     ]
    }
   ],
   "source": [
    "# output the first index of the email\n",
    "print(type(email))\n",
    "print(len(email))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2db54b",
   "metadata": {},
   "source": [
    "In this class, we have been using the notation $\\mathcal{D} = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})\\}$ to represent a dataset. \n",
    "\n",
    "Please answer what $x^{(1)}$ and $y^{(1)}$ are in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67e790e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spam\n",
       "0    4360\n",
       "1    1368\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email.spam.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83583981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text    Subject: naturally irresistible your corporate...\n",
      "spam                                                    1\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(email.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f287d2df",
   "metadata": {},
   "source": [
    "Split the dataset into training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ebff70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Access the 'text' column of the DataFrame\n",
    "email_text = email.text\n",
    "Y = email.spam\n",
    "\n",
    "# Split the data into training and test sets\n",
    "email_train, email_test, y_train, y_test = train_test_split(email_text, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f16130b",
   "metadata": {},
   "source": [
    "Check whether the labels are distributed equally in the training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93bbd597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22804274172530622\n",
      "0.2607086197778953\n"
     ]
    }
   ],
   "source": [
    "print(sum(y_train)/len(y_train))\n",
    "print(sum(y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445f7a1",
   "metadata": {},
   "source": [
    "**Bag of words representation of the data** \n",
    "\n",
    "We start by defining a vocabulary $V$ containing all the possible words we are interested in, e.g.:\n",
    "$$ V = \\{\\text{church}, \\text{doctor}, \\text{fervently}, \\text{purple}, \\text{slow}, ...\\} $$\n",
    "\n",
    "\n",
    "A bag of words representation of a document $x$ is a function $\\phi(x) \\to \\{0,1\\}^{|V|}$ that outputs a feature vector\n",
    "$$\n",
    "\\phi(x) = \\left( \n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\begin{array}{l}\n",
    "\\;\\text{church} \\\\\n",
    "\\;\\text{doctor} \\\\\n",
    "\\;\\text{purple} \\\\\n",
    "\\\\\n",
    "\\;\\text{slow} \\\\\n",
    "\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "of dimension $V$. The $j$-th component $\\phi(x)_j$ equals $1$ if $x$ convains the $j$-th word in $V$ and $0$ otherwise.\n",
    "\n",
    "We will construct the vocabulary dictionary and transform the data at the same time using \"CountVectorizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8224a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define the CountVectorizer object with the desired parameters\n",
    "vectorizer = CountVectorizer(binary = True, max_features=1000)\n",
    "\n",
    "# Fit the vectorizer to the training data and transform the data\n",
    "X_train = vectorizer.fit_transform(email_train).toarray()\n",
    "X_test =  vectorizer.transform(email_test).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c866b",
   "metadata": {},
   "source": [
    "Print the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "47f22528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3837, 1000)\n",
      "(1891, 1000)\n"
     ]
    }
   ],
   "source": [
    "count = vectorizer.vocabulary_\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e150fd7",
   "metadata": {},
   "source": [
    "Check whether comma, \"and\" are in the vocabulary dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1d0c3c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print('comma' in count)\n",
    "print('and' in count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03b2b27",
   "metadata": {},
   "source": [
    "Visualize the feature representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea218f53",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'toarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/msturman00/Documents/GitHub/Applied-Machine-Learning/Lab/Lab NaiveBayes Spam handout.ipynb Cell 19\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/msturman00/Documents/GitHub/Applied-Machine-Learning/Lab/Lab%20NaiveBayes%20Spam%20handout.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Get the vocabulary and feature matrix\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/msturman00/Documents/GitHub/Applied-Machine-Learning/Lab/Lab%20NaiveBayes%20Spam%20handout.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m vocabulary \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mvocabulary_\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/msturman00/Documents/GitHub/Applied-Machine-Learning/Lab/Lab%20NaiveBayes%20Spam%20handout.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m X \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39;49mtoarray()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/msturman00/Documents/GitHub/Applied-Machine-Learning/Lab/Lab%20NaiveBayes%20Spam%20handout.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Create a DataFrame with the feature matrix and column names\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/msturman00/Documents/GitHub/Applied-Machine-Learning/Lab/Lab%20NaiveBayes%20Spam%20handout.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(X, columns\u001b[39m=\u001b[39mvocabulary\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'toarray'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the vocabulary and feature matrix\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "X = X_train.toarray()\n",
    "\n",
    "# Create a DataFrame with the feature matrix and column names\n",
    "df = pd.DataFrame(X, columns=vocabulary.keys())\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "sns.heatmap(corr, cmap='coolwarm')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcaf54a",
   "metadata": {},
   "source": [
    "Check the shape of the transformed feature vector length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8ffcaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X_train.shape[0]\n",
    "p = X_train.shape[1]\n",
    "K = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ce40c5",
   "metadata": {},
   "source": [
    "**Discriminative model**\n",
    "\n",
    "Should we run Linear regression or Logistic regression here? why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f1f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951b4a06",
   "metadata": {},
   "source": [
    "Calculate training and testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8d5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "723de145",
   "metadata": {},
   "source": [
    "The label classes are not evenly distributed in this example, so we need to look at precision and recall.\n",
    "\n",
    "\n",
    "|                   | Predictied Positive   | Predicted Negative         |\n",
    "|-------------------|-----------------------|----------------------------|\n",
    "|Acutal Positive    | True Positive (TP)  | False Negative (FN)  |\n",
    "|Acutal Negative    | False Positive (FP)  | True Negative (TN)  |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "precision = # true positive / (# true positive + # false positive) = # true positive / # predicted to be positive\n",
    "\n",
    "recall = # true positive / (# true positive + # false negative) = # true positive / # actual positive \n",
    "\n",
    "The F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "In the multi-class and multi-label case, this is the average of the F1 score of each class with weighting depending on the average parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff4bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8083f8b6",
   "metadata": {},
   "source": [
    "## Generative Model and Naive Bayes estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f711e6e",
   "metadata": {},
   "source": [
    "Recall that in generative models, we would fit two models on a corpus of emails $x$ with spam/non-spam labels $y$:\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{x}|y=\\text{0}) && \\text{and} && P(\\mathbf{x}|y=\\text{1})\n",
    "\\end{align*}\n",
    "\n",
    "* $P(\\mathbf{x} | y=1)$ *scores* each $\\mathbf{x}$ based on how much it looks like spam.\n",
    "* $P(\\mathbf{x} | y=0)$ *scores* each $\\mathbf{x}$ based on whether it looks like non-spam.\n",
    "\n",
    "We also need to learn the distributions of labels:\n",
    "\n",
    "* $P(y=k)$ encodes our prior beliefs for each class $k$. In spam classification, we might set $P(y=k)$ to the % of data with class $k$.\n",
    "\n",
    "**How do we model these distributions?**\n",
    "\n",
    "* Bernoulli distributions \n",
    "* Model the distribution of a random variable whose outcomes are binary: \n",
    " * e.g., coin flips: if I flip a coin, I see head with probability 0.4\n",
    " * 0.4 is the parameter of the Bernoulli distribution\n",
    " * 0.4 is unknown and we need to learn this parameter\n",
    " \n",
    "* What does unknown parameter mean here: \n",
    " * This means that when I flip this coin, I do not know ahead of the time that I will see head with probability 0.4. \n",
    " * Instead, I only observe realizations, e.g., {H, T, T, H, T, ...}\n",
    " \n",
    "**How do we learn the parameter/mean of a Bernoulli distribution?**\n",
    "* Empirical mean\n",
    "* Coin flip: if I flip the coin 100 times, see 39 heads and 61 tails, then the estimated mean of the Bernoulli distribution is 0.39\n",
    "* Recall that this is the also the optimal solution of the maximum likelihood estimator\n",
    "\n",
    "In the spam detection problem, we use Bernoulli distributions to model:\n",
    "* $P(x_j=1| y=1)$ for all j\n",
    "* $P(x_j=1| y=0)$ for all j\n",
    "* $P(y=1)$ \n",
    "\n",
    "* e.g., $P(x_1=1| y=1) = 0.3$: the probability that I see word \"some\" is 0.3 if the email were a spam\n",
    "* e.g., $P(x_1=1| y=0) = 0.2$: the probability that I see word \"some\" is 0.2 if the email were not a spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9b347",
   "metadata": {},
   "source": [
    "In the training data, \n",
    "* Calculate $P(x_1=1| y=1)$ using the empirical mean: the frequency $x_1$ appeared in the spam emails \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcfa572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8faad964",
   "metadata": {},
   "source": [
    "Calculate $P(x_j=1| y=1)$ for all j, and store the resulting values in a variable called psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a2f06e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a11f04d",
   "metadata": {},
   "source": [
    "Repeat the same exercise, calculate:\n",
    "* Create a variable called psis that has shape K by d.\n",
    "    * The first row contains $P(x_j=1| y=0)$ for all j\n",
    "    * The second row contains $P(x_j=1| y=1)$ for all j\n",
    "* Create a variable called phis with shape K\n",
    "    * stores $P(y=k)$ for all k = 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525bab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "psis = np.zeros([K, d])\n",
    "phis = np.zeros([K])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5342744",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3], [4,5,6]])\n",
    "print(np.mean(a, axis=0))\n",
    "print(np.mean(a, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba3bfd",
   "metadata": {},
   "source": [
    "**Meaning of the first row of psis**\n",
    "* indicate the frequency of the words when the email is not spam\n",
    "\n",
    "**Meaning of the second row of psis**\n",
    "* indicate the frequency of the words when the email is spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d96d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the indices of the elements from the smallest to the biggest\n",
    "sorted_spamicity = np.argsort(psis[1])\n",
    "# retrive the top 10 most frequent words in the category of spam\n",
    "print(sorted_spamicity[-10:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af2c91f",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "Given a new $x'$, we return the most likely class to have generated it:\n",
    "\n",
    "\\begin{align*}\n",
    "\\arg \\max_k P_\\theta(y=k | x') & = \\arg \\max_k  \\frac{P_\\theta(x' | y=k) P_\\theta(y=k)}{P_\\theta(x')} \\\\\n",
    "& = \\arg \\max_k P_\\theta(x' | y=k) P_\\theta(y=k),\n",
    "\\end{align*}\n",
    "\n",
    "where we have applied Bayes' rule in the first line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f518b4a",
   "metadata": {},
   "source": [
    "## Naive Bayes Assumption\n",
    "\n",
    "The Naive Bayes assumption is a __general technique__ that can be used with any $d$-dimensional $x$ to construct tractable models $P(x|y)$.\n",
    "* We simplify the model for $x$ as:\n",
    "$$ P(x|y) = \\prod_{j=1}^d P(x_j \\mid y) $$\n",
    "\n",
    "Thus, the prediction problem boils down to\n",
    "\\begin{align*}\n",
    "\\arg \\max_k P(y=k | x') \n",
    "& = \\arg \\max_k P(x' | y=k) P(y=k),\\\\\n",
    "& =  \\arg \\max_k \\prod_{j=1}^d P(x_j' | y=k;\\psi_{jk}) \\phi_k\n",
    "\\end{align*}\n",
    "\n",
    "**Log-Likelihood**\n",
    "\\begin{align*}\n",
    "\\arg \\max_k \\log P(y=k | x') \n",
    "& =  \\arg \\max_k \\sum_{j=1}^d \\log P(x_j' | y=k;\\psi_{jk}) \\phi_k\\\\\n",
    "& = \\arg \\max_k \\sum_{j=1}^d \\log P(x_j' | y=k;\\psi_{jk}) + \\log \\phi_k\n",
    "\\end{align*}\n",
    "\n",
    "**How do we calculate $P(x_j' | y=k;\\psi_{jk})$?**\n",
    "* If I have a coin with head probability 0.4, what is the probability that I observe 0?\n",
    "    * 0.4\n",
    "* What is the probability that I observe 1?\n",
    "    * 0.6\n",
    "\n",
    "Similarly, here we have that \n",
    "* $P(x_j'=1 | y=k;\\psi_{jk}) = \\psi_{jk}$\n",
    "* $P(x_j'=0 | y=k;\\psi_{jk}) = 1 - \\psi_{jk}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf31318d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82871ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c071afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember: P(x|y) indicates P(x1|y)*P(x2|y)* ... * P(xn|y)\n",
    "# Let's do P(x|y=0) first\n",
    "def spam_predict(x, psis, phis, K=2):\n",
    "    \"\"\"This returns class assignments and scores under the NB \n",
    "    model. We compute \\arg\\max_y p(y|x) as \\arg\\max_y p(x|y)p(y)\n",
    "    \"\"\"\n",
    "    n, d = x.shape\n",
    "    psis = psis.clip(1e-14, 1-1e-14)\n",
    "    log_py = np.log(phis)\n",
    "    score = np.zeros((n, K))\n",
    "    for i in range(n):\n",
    "        for k in range(K):\n",
    "            # fill in the calculation of score here\n",
    "\n",
    "    return score.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06697cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237864ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f9192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767053c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7223ca0b",
   "metadata": {},
   "source": [
    "Check whether a word has only appeared in the y=0 but not y=1 or vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce57ae43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b419517d",
   "metadata": {},
   "source": [
    "Modify the dictionary to remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ebd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82cfaa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10d7b356",
   "metadata": {},
   "source": [
    "Process stop words in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3420c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --user nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a551c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4c9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "137dcd9f",
   "metadata": {},
   "source": [
    "Preprocess the data to keep only the stem of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c67d946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "new_dict = {}\n",
    "value = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774df196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7425ed63",
   "metadata": {},
   "source": [
    "**Written Exercises**\n",
    "\n",
    "**Naive Bayes with Binary Features.** Consider a group of 50 Cornell Students. \n",
    "20 of them are Master's students, while the rest 30 of them are PhD students. \n",
    "There are 5 Master's students who bike, and there are 5 Master's students who ski.\n",
    "On the other hand, 20 PhD students bike, and 15 PhD students ski. \n",
    "\n",
    "We can formulate this as a machine learning problem by modeling the students with features $x =(x_1, x_2) \\in \\{0,1\\}^2$, where $x_1$ is a binary indicator of whether the students bike and $x_2$ is a binary indicator of whether they ski, and the target $y$ equals $1$ if they are PhD students and $0$ if they are Master's students.\n",
    "\n",
    "* Please elaborate in this context what is the Naive Bayes assumption.\n",
    "* With the Naive Bayes assumption,\n",
    "    % (probability of biking and skiing are conditionally independent given a study program), \n",
    "    find the probability of a student in this group who neither bikes or skis being a Master's student\n",
    "    \n",
    "* Suppose we know that every PhD who skis also bikes. Does it make sense to still assume that probability of biking and skiing are conditionally independent for a PhD student? If not, how would your answer to part (b) change with this knowledge (you can still assume probability of biking and skiing are conditionally independent for a Master's student)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927ea1c8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
